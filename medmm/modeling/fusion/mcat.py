import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from medmm.modeling.ops import SNN_Block, MultiheadAttention, Attn_Net_Gated, BilinearFusion

from .build import FUSION_REGISTRY
from .basefusion import BaseFusion

class MCAT(BaseFusion):
    def __init__(self, 
                 fusion: str='concat', 
                 path_input_dim=512, 
                 omic_sizes=[200, 200, 200, 200, 200],   # [100, 200, 300, 400, 500, 372],
                 model_size_wsi: str='small', 
                 model_size_omic: str='small', 
                 dropout: float=0.10):
        super(MCAT, self).__init__()
        
        self.fusion = fusion
        self.omic_sizes = omic_sizes
        self.size_dict_WSI = {"small": [path_input_dim, 256, 256], "big": [path_input_dim, 512, 384]}
        self.size_dict_omic = {'small': [256, 256], 'big': [1024, 1024, 1024, 256]}
        
        ### FC Layer over WSI bag
        size = self.size_dict_WSI[model_size_wsi]
        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]
        fc.append(nn.Dropout(dropout))
        self.wsi_net = nn.Sequential(*fc)
        
        ### Constructing Genomic SNN
        hidden = self.size_dict_omic[model_size_omic]
        sig_networks = []
        for input_dim in omic_sizes:
            fc_omic = [SNN_Block(dim1=input_dim, dim2=hidden[0])]
            for i, _ in enumerate(hidden[1:]):
                fc_omic.append(SNN_Block(dim1=hidden[i], dim2=hidden[i+1], dropout=dropout))
            sig_networks.append(nn.Sequential(*fc_omic))
        self.sig_networks = nn.ModuleList(sig_networks)
     

        ### Multihead Attention
        self.coattn = MultiheadAttention(embed_dim=256, num_heads=1)

        ### Path Transformer + Attention Head
        path_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout, activation='relu')
        self.path_transformer = nn.TransformerEncoder(path_encoder_layer, num_layers=2)
        self.path_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)
        self.path_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])
        
        ### Omic Transformer + Attention Head
        omic_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout, activation='relu')
        self.omic_transformer = nn.TransformerEncoder(omic_encoder_layer, num_layers=2)
        self.omic_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)
        self.omic_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])
        
        ### Fusion Layer
        if self.fusion == 'concat':
            self.mm = nn.Sequential(*[nn.Linear(256*2, size[2]), nn.ReLU(), nn.Linear(size[2], size[2]), nn.ReLU()])
        elif self.fusion == 'bilinear':
            self.mm = BilinearFusion(dim1=256, dim2=256, scale_dim1=8, scale_dim2=8, mmhid=256)
        else:
            self.mm = None
            
        self._out_features = hidden[-1]


    def forward(self, x_path, x_omic):
        omic_sizes = self.omic_sizes
        x_omics =  [x_omic[:, sum(omic_sizes[:i]):sum(omic_sizes[:i+1])] for i in range(len(omic_sizes))]
        # import pdb;pdb.set_trace()
        h_path_bag = self.wsi_net(x_path) ### path embeddings are fed through a FC layer
        h_omic = [self.sig_networks[idx].forward(sig_feat) for idx, sig_feat in enumerate(x_omics)] ### each omic signature goes through it's own FC layer
        h_omic_bag = torch.stack(h_omic) ### omic embeddings are stacked (to be used in co-attention)

        # Coattn
        h_path_coattn, A_coattn = self.coattn(h_omic_bag, h_path_bag, h_path_bag)


        ### Path
        h_path_trans = self.path_transformer(h_path_coattn)
        A_path, h_path = self.path_attention_head(h_path_trans.squeeze(1))
        A_path = torch.transpose(A_path, 1, 0)
        h_path = torch.mm(F.softmax(A_path, dim=1) , h_path)
        h_path = self.path_rho(h_path)
        
        ### Omic
        h_omic_trans = self.omic_transformer(h_omic_bag)
        A_omic, h_omic = self.omic_attention_head(h_omic_trans.squeeze(1))
        A_omic = torch.transpose(A_omic, 1, 0)
        h_omic = torch.mm(F.softmax(A_omic, dim=1) , h_omic)
        h_omic = self.omic_rho(h_omic)
        
        # import pdb;pdb.set_trace()
        if self.fusion == 'bilinear':
            out = self.mm(h_path, h_omic)
        elif self.fusion == 'concat':
            out = self.mm(torch.cat([h_path, h_omic], axis=1))
                
        attention_scores = {'coattn': A_coattn, 'path': A_path, 'omic': A_omic}
        
        return out, attention_scores


    # def captum(self, x_path, x_omic1, x_omic2, x_omic3, x_omic4, x_omic5, x_omic6):
    #     #x_path = torch.randn((10, 500, 1024))
    #     #x_omic1, x_omic2, x_omic3, x_omic4, x_omic5, x_omic6 = [torch.randn(10, size) for size in omic_sizes]
    #     x_omic = [x_omic1, x_omic2, x_omic3, x_omic4, x_omic5, x_omic6]
    #     h_path_bag = self.wsi_net(x_path)#.unsqueeze(1) ### path embeddings are fed through a FC layer
    #     h_path_bag = torch.reshape(h_path_bag, (500, 10, 256))
    #     h_omic = [self.sig_networks[idx].forward(sig_feat) for idx, sig_feat in enumerate(x_omic)] ### each omic signature goes through it's own FC layer
    #     h_omic_bag = torch.stack(h_omic) ### omic embeddings are stacked (to be used in co-attention)

    #     # Coattn
    #     h_path_coattn, A_coattn = self.coattn(h_omic_bag, h_path_bag, h_path_bag)

    #     ### Path
    #     h_path_trans = self.path_transformer(h_path_coattn)
    #     h_path_trans = torch.reshape(h_path_trans, (10, 6, 256))
    #     A_path, h_path = self.path_attention_head(h_path_trans)
    #     A_path = F.softmax(A_path.squeeze(dim=2), dim=1).unsqueeze(dim=1)
    #     h_path = torch.bmm(A_path, h_path).squeeze(dim=1)

    #     ### Omic
    #     h_omic_trans = self.omic_transformer(h_omic_bag)
    #     h_omic_trans = torch.reshape(h_omic_trans, (10, 6, 256))
    #     A_omic, h_omic = self.omic_attention_head(h_omic_trans)
    #     A_omic = F.softmax(A_omic.squeeze(dim=2), dim=1).unsqueeze(dim=1)
    #     h_omic = torch.bmm(A_omic, h_omic).squeeze(dim=1)

    #     if self.fusion == 'bilinear':
    #         h = self.mm(h_path.unsqueeze(dim=0), h_omic.unsqueeze(dim=0)).squeeze()
    #     elif self.fusion == 'concat':
    #         h = self.mm(torch.cat([h_path, h_omic], axis=1))

    #     logits  = self.classifier(h)
    #     hazards = torch.sigmoid(logits)
    #     S = torch.cumprod(1 - hazards, dim=1)

    #     risk = -torch.sum(S, dim=1)
    #     return risk


    
    
@FUSION_REGISTRY.register()
def mcat(**kwargs):
    return MCAT(**kwargs)